"""
·ª®ng d·ª•ng FastAPI cho D·ªãch v·ª• T·∫°o Kh·∫£o s√°t (MAIN.py)
Cung c·∫•p t·∫°o kh·∫£o s√°t th√¥ng minh b·∫±ng AI s·ª≠ d·ª•ng Gemini API
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
import logging
import os
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from datetime import datetime
from pydantic import BaseModel
from app.core.gemini_client import GeminiOverloadedError
import re, math, random
from pathlib import Path
from collections import Counter

# Load .env t·ª´ th∆∞ m·ª•c g·ªëc c·ªßa project
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

from math import ceil
SURVEY_AI_FALLBACK = os.getenv("SURVEY_AI_FALLBACK", "1") != "0"


from .models.survey_schemas import (
    SurveyGenerationRequest, 
    SurveyGenerationResponse,
    SurveyGenerationError,
    QuestionSchema,
    GeneratedSurveyResponse,
    CATEGORY_MAPPING
)
from .core.gemini_client import create_gemini_client, GeminiClient

def suggest_followups(q_text: str, q_type: str) -> list[str]:
    t = (q_type or "").lower()
    if t == "open_ended":
        return ["B·∫°n c√≥ th·ªÉ m√¥ t·∫£ v√≠ d·ª• c·ª• th·ªÉ kh√¥ng?", "Y·∫øu t·ªë n√†o ·∫£nh h∆∞·ªüng l·ªõn nh·∫•t ƒë·∫øn c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n?"]
    if t == "rating":
        return ["L√Ω do b·∫°n ch·ªçn m·ª©c ƒëi·ªÉm ƒë√≥ l√† g√¨?", "Nh·ªØng c·∫£i thi·ªán n√†o s·∫Ω gi√∫p b·∫°n n√¢ng m·ª©c ƒë√°nh gi√°?"]
    if t in ("multiple_choice", "single_choice"):
        return ["Y·∫øu t·ªë n√†o quan tr·ªçng nh·∫•t khi b·∫°n c√¢n nh·∫Øc l·ª±a ch·ªçn tr√™n?", "B·∫°n s·∫Ω ƒë·ªÅ xu·∫•t th√™m l·ª±a ch·ªçn n√†o kh√¥ng?"]
    if t == "ranking":
        return ["T·∫°i sao b·∫°n x·∫øp h·∫°ng m·ª•c A cao h∆°n m·ª•c B?", "ƒêi·ªÅu g√¨ c√≥ th·ªÉ thay ƒë·ªïi th·ª© t·ª± ∆∞u ti√™n c·ªßa b·∫°n?"]
    if t == "date_time":
        return ["Khung gi·ªù thay th·∫ø ph√π h·ª£p v·ªõi b·∫°n?", "B·∫°n c√≥ c·∫ßn ƒë·∫∑t nh·∫Øc nh·ªü kh√¥ng?"]
    if t == "boolean_":
        return ["ƒêi·ªÅu g√¨ d·∫´n ƒë·∫øn quy·∫øt ƒë·ªãnh c√≥/kh√¥ng c·ªßa b·∫°n?", "Trong tr∆∞·ªùng h·ª£p n√†o b·∫°n s·∫Ω ch·ªçn ng∆∞·ª£c l·∫°i?"]
    if t == "file_upload":
        return ["B·∫°n c√≥ t√†i li·ªáu minh ho·∫°/ƒë√≠nh k√®m b·ªï sung kh√¥ng?", "B·∫°n c·∫ßn h∆∞·ªõng d·∫´n v·ªÅ ƒë·ªãnh d·∫°ng t·ªáp kh√¥ng?"]
    return []

PARALLEL_WORKERS = int(os.getenv("SURVEY_AI_WORKERS", "2"))
SINGLE_TIMEOUT   = int(os.getenv("SURVEY_AI_SINGLE_TIMEOUT", "18"))
MIN_SCORE        = int(os.getenv("SURVEY_AI_MIN_SCORE", "55"))

def _gen_one_safe(client: GeminiClient, req: SurveyGenerationRequest, qtype: Optional[str]):
    try:
        # G·ªçi generate_one_question v·ªõi ƒë√∫ng signature
        result = client.generate_one_question(
            title=req.title,
            category=req.category_name or "general",
            question_type=qtype or "open_ended",
            ai_prompt=req.ai_prompt or "",
            previous_question=""
        )
        if not result or not result.get("question_text"):
            logger.warning(f"‚ö†Ô∏è Empty result from generate_one_question for type={qtype}")
        return result
    except Exception as e:
        logger.error(f"‚ùå Exception in _gen_one_safe for type={qtype}: {e}", exc_info=True)
        return {}


def parallel_generate_exact_n(client: GeminiClient, req: SurveyGenerationRequest) -> list[dict]:
    """Sinh ƒë√∫ng N c√¢u b·∫±ng c√°ch b·∫Øn nhi·ªÅu request 1-c√¢u song song theo 'waves' cho t·ªõi khi ƒë·ªß."""
    N = int(req.number_of_questions)
    priorities = getattr(req, "question_type_priorities", None)
    logger.info(f"üéØ Priority selection: N={N}, priorities={priorities}")
    target_types = choose_target_types(N, priorities)
    logger.info(f"üìä Target types generated: {target_types}")

    results: list[dict] = []
    seen = set()
    
    # üî• SAFETY LIMITS
    MIN_ACCEPTABLE = max(3, int(N * 0.6))  # T·ªëi thi·ªÉu 60% s·ªë c√¢u ho·∫∑c 3 c√¢u
    MAX_RETRIES_PER_QUESTION = 1  # Ch·ªâ retry 1 l·∫ßn/c√¢u
    QUOTA_ERRORS = 0  # Track s·ªë l·∫ßn g·∫∑p 429
    MAX_QUOTA_ERRORS = 3  # D·ª´ng sau 3 l·∫ßn 429

    wave = 0
    while len(results) < N and wave < 5:  # ‚úÖ Gi·∫£m t·ª´ 10 ‚Üí 5 waves
        wave += 1

        # L·∫≠p danh s√°ch lo·∫°i c·∫ßn sinh ·ªü ƒë·ª£t n√†y (c√≤n thi·∫øu lo·∫°i n√†o th√¨ ƒë·∫©y tr∆∞·ªõc)
        missing = N - len(results)
        types_needed = target_types[len(results): len(results) + missing]
        # tƒÉng th√™m m·ªôt ch√∫t ƒë·ªÉ c√≥ d∆∞ m√† l·ªçc
        extra = choose_target_types(max(0, missing//2), priorities)
        wanted_types = types_needed + extra

        # ‚úÖ FIX: Gi·ªØ ƒë√∫ng th·ª© t·ª± b·∫±ng c√°ch d√πng list index thay v√¨ as_completed
        with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as ex:
            # Submit v·ªõi index v√† type ƒë·ªÉ track
            indexed_futs = [(i, t, ex.submit(_gen_one_safe, client, req, t)) for i, t in enumerate(wanted_types)]
            
            # ƒê·ª£i t·∫•t c·∫£ futures ho√†n th√†nh v√† sort theo index g·ªëc
            completed = []
            for idx, expected_type, fut in indexed_futs:
                try:
                    one = fut.result(timeout=SINGLE_TIMEOUT) or {}
                    completed.append((idx, expected_type, one))
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to generate question at index {idx} (type={expected_type}): {e}")
                    continue
            
            # Sort theo index ƒë·ªÉ gi·ªØ ƒë√∫ng th·ª© t·ª± priorities
            completed.sort(key=lambda x: x[0])
            
            for idx, expected_type, one in completed:
                q_text = (one.get("question_text") or "").strip()
                if not q_text:
                    logger.warning(f"‚ö†Ô∏è Question at index {idx} has no text (expected={expected_type})")
                    
                    # üî• CHECK: D·ª´ng retry n·∫øu ƒë√£ v∆∞·ª£t quota error limit
                    if QUOTA_ERRORS >= MAX_QUOTA_ERRORS:
                        logger.error(f"üõë STOP: ƒê√£ g·∫∑p {QUOTA_ERRORS} quota errors. D·ª´ng retry ƒë·ªÉ tr√°nh spam API!")
                        break
                    
                    # üî• CHECK: D·ª´ng retry n·∫øu ƒë√£ ƒë·ªß minimum acceptable
                    if len(results) >= MIN_ACCEPTABLE:
                        logger.info(f"‚úÖ ƒê√£ ƒë·ªß {len(results)}/{N} c√¢u (minimum: {MIN_ACCEPTABLE}). Skip retry.")
                        continue
                    
                    # ‚ú® RETRY: Ch·ªâ retry 1 l·∫ßn/c√¢u v·ªõi priority
                    if priorities and expected_type and wave <= 2:  # Ch·ªâ retry trong 2 waves ƒë·∫ßu
                        logger.info(f"üîÑ Retrying generation for type={expected_type} at index {idx}")
                        try:
                            retry_result = _gen_one_safe(client, req, expected_type)
                            if retry_result and retry_result.get("question_text"):
                                one = retry_result
                                q_text = one.get("question_text", "").strip()
                                logger.info(f"‚úÖ Retry successful for {expected_type}")
                            else:
                                logger.warning(f"‚ùå Retry failed for {expected_type}, skipping")
                                QUOTA_ERRORS += 1  # Increment on empty result
                                continue
                        except Exception as e:
                            error_msg = str(e).lower()
                            if "429" in error_msg or "quota" in error_msg:
                                QUOTA_ERRORS += 1
                                logger.error(f"üö´ Quota exceeded during retry ({QUOTA_ERRORS}/{MAX_QUOTA_ERRORS})")
                            continue
                    else:
                        continue

                key = q_text.lower()
                if key in seen:
                    logger.debug(f"üîÑ Duplicate question skipped at index {idx}")
                    continue

                q_type = _normalize_type(one.get("question_type") or "open_ended")
                logger.info(f"üìù Question {idx}: expected={expected_type}, got={q_type}, text='{q_text[:50]}...'")
                
                options = one.get("options")

                if q_type in ("multiple_choice", "single_choice", "ranking"):
                    opts = _coerce_option_list(options)
                    if q_type == "ranking" and len(opts) < 5:
                        for j in range(len(opts)+1, 6):
                            opts.append(f"M·ª•c {j}")
                    options = opts
                else:
                    options = None

                issues = validate_question_text(q_text)
                score  = score_question(q_text)

                # ‚ö†Ô∏è CRITICAL: V·ªõi priorities, B·∫ÆT BU·ªòC type ph·∫£i match
                if priorities and len(results) < N:
                    # Ki·ªÉm tra type c√≥ match v·ªõi expected kh√¥ng
                    if q_type != expected_type:
                        logger.warning(f"‚ùå Type mismatch at {idx}: expected={expected_type}, got={q_type}. SKIPPING.")
                        continue
                    
                    # V·ªõi priorities, ∆∞u ti√™n ƒë·ªß s·ªë l∆∞·ª£ng h∆°n quality
                    if not issues:  # Ch·ªâ b·ªè n·∫øu c√≥ issues nghi√™m tr·ªçng
                        logger.info(f"‚úÖ Accepted question {idx} with type={q_type} (priority mode)")
                        results.append({
                            "question_text": q_text,
                            "type": q_type,
                            "options": options,
                            "score": score,
                            "issues": issues
                        })
                        seen.add(key)
                    else:
                        logger.warning(f"‚ö†Ô∏è Question {idx} has issues: {issues}")
                else:
                    # Kh√¥ng c√≥ priorities, gi·ªØ logic c≈© (filter theo score)
                    if not issues and score >= MIN_SCORE:
                        results.append({
                            "question_text": q_text,
                            "type": q_type,
                            "options": options,
                            "score": score,
                            "issues": issues
                        })
                        seen.add(key)

                if len(results) >= N:
                    logger.info(f"‚úÖ ƒê√£ ƒë·ªß {N} c√¢u h·ªèi, stop generating.")
                    break
            
            # üî• BREAK: D·ª´ng wave loop n·∫øu g·∫∑p qu√° nhi·ªÅu quota errors
            if QUOTA_ERRORS >= MAX_QUOTA_ERRORS:
                logger.error(f"üõë STOP WAVES: ƒê√£ g·∫∑p {QUOTA_ERRORS} quota errors. D·ª´ng t·∫•t c·∫£ generation!")
                break
            
            # üî• EARLY EXIT: N·∫øu ƒë√£ ƒë·ªß minimum v√† ƒëang g·∫∑p v·∫•n ƒë·ªÅ API
            if len(results) >= MIN_ACCEPTABLE and QUOTA_ERRORS > 0:
                logger.warning(f"‚ö†Ô∏è EARLY EXIT: ƒê√£ c√≥ {len(results)}/{N} c√¢u (ƒë·ªß minimum {MIN_ACCEPTABLE}). D·ª´ng ƒë·ªÉ ti·∫øt ki·ªám API.")
                break

    # Log final results
    logger.info(f"üéØ Final result: generated {len(results)}/{N} questions")
    if len(results) < N:
        if len(results) >= MIN_ACCEPTABLE:
            logger.warning(f"‚ö†Ô∏è Thi·∫øu {N - len(results)} c√¢u nh∆∞ng ƒë√£ ƒë·∫°t minimum acceptable ({len(results)}/{MIN_ACCEPTABLE})")
        else:
            logger.error(f"‚ùå Ch·ªâ t·∫°o ƒë∆∞·ª£c {len(results)}/{N} c√¢u (minimum: {MIN_ACCEPTABLE})")
    
    return results[:N]

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# T·∫°o ·ª©ng d·ª•ng FastAPI
app = FastAPI(
    title="SmartSurvey - D·ªãch v·ª• T·∫°o Kh·∫£o s√°t",
    description="API d·ªãch v·ª• t·∫°o kh·∫£o s√°t th√¥ng minh s·ª≠ d·ª•ng AI",
    version="1.0.0"
)

# 1) Prompt templates (5+ industry templates) + DEFAULT
DEFAULT_TEMPLATE = """B·∫°n l√† chuy√™n gia nghi√™n c·ª©u th·ªã tr∆∞·ªùng.
H√£y t·∫°o {n} c√¢u h·ªèi kh·∫£o s√°t ch·∫•t l∆∞·ª£ng cao cho ch·ªß ƒë·ªÅ: "{title}".
Lo·∫°i c√¢u h·ªèi c√≥ th·ªÉ d√πng: {types}.
Y√™u c·∫ßu:
- R√µ r√†ng, m·ªôt √Ω h·ªèi m·ªôt ƒëi·ªÅu
- Kh√¥ng d·∫´n d·∫Øt, kh√¥ng thi√™n ki·∫øn
- C√°c l·ª±a ch·ªçn (v·ªõi c√¢u multiple-choice/single-choice) c√¢n b·∫±ng, kh√¥ng tr√πng l·∫∑p
- V·ªõi c√¢u ranking: cung c·∫•p 5-7 m·ª•c ƒë·ªÉ x·∫øp h·∫°ng, kh√¥ng tr√πng √Ω
- V·ªõi rating: n√™u r√µ thang ƒëi·ªÉm (1-5)
- V·ªõi open-ended: tr√°nh c√¢u h·ªèi qu√° r·ªông, c√≥ g·ª£i √Ω ph·∫°m vi tr·∫£ l·ªùi

Tr·∫£ v·ªÅ JSON theo schema ƒë√£ ƒë∆∞·ª£c m√¥ t·∫£ (questions: text, type, options n·∫øu c√≥)."""

INDUSTRY_TEMPLATES: Dict[str, str] = {
    "ecommerce": """B·∫°n l√† chuy√™n gia CX cho s√†n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠.
T·∫°o {n} c√¢u h·ªèi cho survey "{title}".
Lo·∫°i: {types}.
Nh·∫•n m·∫°nh: tr·∫£i nghi·ªám mua h√†ng, thanh to√°n, giao h√†ng, ƒë·ªïi tr·∫£, CSKH, l√≤ng trung th√†nh.
ƒê·∫£m b·∫£o r√µ r√†ng, kh√¥ng leading, ph√π h·ª£p ng·ªØ c·∫£nh eCommerce.
Tr·∫£ v·ªÅ JSON theo schema.""",

    "saas": """B·∫°n l√† chuy√™n gia Product cho SaaS B2B.
T·∫°o {n} c√¢u h·ªèi cho survey "{title}".
Lo·∫°i: {types}.
Nh·∫•n m·∫°nh: onboarding, feature usability, pricing, support, performance, ROI.
Tr·∫£ v·ªÅ JSON theo schema.""",

    "education": """B·∫°n l√† chuy√™n gia kh·∫£o th√≠ trong gi√°o d·ª•c.
T·∫°o {n} c√¢u h·ªèi cho survey "{title}".
Lo·∫°i: {types}.
Nh·∫•n m·∫°nh: ch·∫•t l∆∞·ª£ng gi·∫£ng d·∫°y, t√†i li·ªáu, LMS, ƒë√°nh gi√°, h·ªó tr·ª£ h·ªçc t·∫≠p.
Tr·∫£ v·ªÅ JSON theo schema.""",

    "healthcare": """B·∫°n l√† chuy√™n gia tr·∫£i nghi·ªám b·ªánh nh√¢n (PX).
T·∫°o {n} c√¢u h·ªèi cho survey "{title}".
Lo·∫°i: {types}.
Nh·∫•n m·∫°nh: ƒë·∫∑t l·ªãch, ti·∫øp ƒë√≥n, ch·∫©n ƒëo√°n, ƒëi·ªÅu tr·ªã, chƒÉm s√≥c, th√¥ng tin r√µ r√†ng.
Tr·∫£ v·ªÅ JSON theo schema.""",

    "finance": """B·∫°n l√† chuy√™n gia tr·∫£i nghi·ªám kh√°ch h√†ng cho d·ªãch v·ª• t√†i ch√≠nh.
T·∫°o {n} c√¢u h·ªèi cho survey "{title}".
Lo·∫°i: {types}.
Nh·∫•n m·∫°nh: m·ªü t√†i kho·∫£n, giao d·ªãch, ph√≠, b·∫£o m·∫≠t, h·ªó tr·ª£, app/mobile banking.
Tr·∫£ v·ªÅ JSON theo schema.""",
}
SUPPORTED_TYPES = ["multiple_choice","single_choice","ranking","rating","open_ended","boolean_","date_time","file_upload"]

def choose_target_types(n: int, priorities: list[str] | None = None) -> list[str]:
    """
    Ch·ªçn danh s√°ch lo·∫°i c√¢u h·ªèi theo m·ª©c ƒë·ªô ∆∞u ti√™n.
    
    Args:
        n: S·ªë l∆∞·ª£ng c√¢u h·ªèi c·∫ßn t·∫°o
        priorities: Danh s√°ch lo·∫°i c√¢u h·ªèi ∆∞u ti√™n theo th·ª© t·ª± (n·∫øu None s·∫Ω d√πng m·∫∑c ƒë·ªãnh)
    
    Returns:
        Danh s√°ch lo·∫°i c√¢u h·ªèi theo th·ª© t·ª± ∆∞u ti√™n
    """
    # N·∫øu kh√¥ng c√≥ priorities, d√πng m·∫∑c ƒë·ªãnh v·ªõi rating, single_choice, multiple_choice ∆∞u ti√™n cao h∆°n
    if not priorities or len(priorities) == 0:
        # M·∫∑c ƒë·ªãnh: ∆∞u ti√™n rating, single_choice, multiple_choice (xu·∫•t hi·ªán nhi·ªÅu h∆°n)
        wheel = [
            "rating", "single_choice", "multiple_choice",  # ∆Øu ti√™n cao - l·∫∑p l·∫°i 2 l·∫ßn
            "rating", "single_choice", "multiple_choice",
            "ranking", "boolean_", "open_ended",  # ∆Øu ti√™n trung b√¨nh
            "date_time", "file_upload"  # ∆Øu ti√™n th·∫•p
        ]
    else:
        # ‚úÖ GUARANTEED DISTRIBUTION: M·ªói priority type PH·∫¢I xu·∫•t hi·ªán √≠t nh·∫•t 1 l·∫ßn
        priority_count = len(priorities)
        
        # B∆Ø·ªöC 1: ƒê·∫£m b·∫£o m·ªói priority c√≥ √≠t nh·∫•t 1 c√¢u
        result = list(priorities)  # Copy ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng tham s·ªë g·ªëc
        remaining_slots = n - priority_count
        
        if remaining_slots <= 0:
            # N·∫øu s·ªë c√¢u <= s·ªë priorities ‚Üí ch·ªâ l·∫•y n ƒë·∫ßu ti√™n
            return result[:n]
        
        # B∆Ø·ªöC 2: Ph√¢n ph·ªëi c√°c slot c√≤n l·∫°i theo weighted priority
        # T·∫°o weighted wheel: priority c√†ng cao (index c√†ng th·∫•p) ‚Üí xu·∫•t hi·ªán c√†ng nhi·ªÅu
        wheel = []
        for idx, qtype in enumerate(priorities):
            # Weight = priority_count - idx (c√†ng ƒë·∫ßu c√†ng n·∫∑ng)
            # VD: 4 priorities -> weights = [4, 3, 2, 1]
            weight = priority_count - idx
            wheel.extend([qtype] * weight)
        
        # B∆Ø·ªöC 3: Random sample t·ª´ wheel ƒë·ªÉ fill c√°c slot c√≤n l·∫°i
        import random
        additional = random.choices(wheel, k=remaining_slots)
        result.extend(additional)
        
        # Shuffle ƒë·ªÉ tr√°nh pattern qu√° r√µ r√†ng (nh∆∞ng v·∫´n gi·ªØ distribution)
        random.shuffle(result)
        
        logger.info(f"üìä Distribution: {dict(Counter(result))}")
        return result[:n]

def _ensure_min_options(qtype: str, options: list | None) -> list[dict]:
    # Chu·∫©n ho√° v·ªÅ list[dict(option_text, display_order)] nh∆∞ng KH√îNG set c·ª©ng A/B/C/D cho MCQ/Single
    opts = options or []
    normalized = []
    for i, o in enumerate(opts):
        if isinstance(o, dict) and (o.get("option_text") or "").strip():
            normalized.append({"option_text": o["option_text"].strip(), "display_order": i+1})
        elif isinstance(o, str) and o.strip():
            normalized.append({"option_text": o.strip(), "display_order": i+1})

    if qtype == "ranking":
        # Ranking c·∫ßn ƒë·ªß item ƒë·ªÉ s·∫Øp th·ª© h·∫°ng m∆∞·ª£t
        if len(normalized) < 5:
            need = 5 - len(normalized)
            normalized += [{"option_text": f"M·ª•c {k}", "display_order": len(normalized)+k} for k in range(1, need+1)]
    elif qtype in ("multiple_choice", "single_choice"):
        # KH√îNG t·ª± th√™m b·∫•t k·ª≥ l·ª±a ch·ªçn n√†o; ƒë·ªÉ r·ªóng n·∫øu AI ch∆∞a sinh ƒë·ªß
        pass
    else:
        # C√°c lo·∫°i kh√°c kh√¥ng c√≥ options
        normalized = []

    # Re-index display_order
    for i, o in enumerate(normalized, start=1):
        o["display_order"] = i
    return normalized


def enforce_type_mix(generated: list[dict], target_types: list[str]) -> list[dict]:
    map_alias = {
        "boolean":"boolean_","yes_no":"boolean_","true_false":"boolean_",
        "date":"date_time","datetime":"date_time","time":"date_time","time_picker":"date_time",
        "file":"file_upload","upload":"file_upload","attachment":"file_upload",
        "single":"single_choice","radio":"single_choice",
        "mcq":"multiple_choice","multiple":"multiple_choice","multi_choice":"multiple_choice","checkbox":"multiple_choice"
    }
    out = []
    for i, q in enumerate(generated):
        want = target_types[i] if i < len(target_types) else None
        raw_t = (q.get("type") or q.get("question_type") or "").strip().lower().replace("-", "_").replace(" ", "_")
        got = map_alias.get(raw_t, raw_t) or "open_ended"
        qtype = want or got
        if want and got != want:
            qtype = want
        qtext = q.get("question_text") or q.get("text") or q.get("question") or f"C√¢u h·ªèi {i+1}"
        options = _ensure_min_options(qtype, q.get("options") or q.get("options_list") or [])
        str_opts = [o["option_text"] for o in options] if options else None
        out.append({"question_text": qtext, "type": qtype, "options": str_opts})
    return out

# 2) Helpers: chu·∫©n ho√° category, build prompt, validate + score
def normalize_category(cat) -> str:
    from app.models.survey_schemas import CATEGORY_MAPPING
    if cat is None:
        return "general"
    if isinstance(cat, int) and cat in CATEGORY_MAPPING:
        return str(CATEGORY_MAPPING[cat]).strip().lower()
    return str(cat).strip().lower()

def _coerce_option_texts(options) -> list[str]:
    """
    Nh·∫≠n m·ªçi ki·ªÉu: None / list[str] / list[dict] / t·∫°p.
    Tr·∫£ v·ªÅ list[str] s·∫°ch, kh√¥ng r·ªóng, t·ªëi ƒëa 7 m·ª•c.
    """
    if not options:
        return []
    out = []
    for o in options:
        if isinstance(o, dict):
            txt = (o.get("option_text")
                   or o.get("text")
                   or o.get("label")
                   or o.get("value")
                   or "").strip()
        else:
            txt = str(o).strip()
        # g·ª° noise ki·ªÉu "option_text: 'xxx', display_order: 2"
        if txt.lower().startswith("option_text"):
            m = re.search(r"option_text\s*[:=]\s*['\"]([^'\"]+)['\"]", txt, flags=re.I)
            if m:
                txt = m.group(1).strip()
        if txt:
            out.append(txt)

    # lo·∫°i tr√πng + c·∫Øt ng·∫Øn
    seen, uniq = set(), []
    for t in out:
        if t.lower() not in seen:
            seen.add(t.lower())
            uniq.append(t)
    return uniq[:7]


def regen_low_quality_questions(
    title: str,
    category: str,
    items: list[dict],
    client,  # gemini_client instance
    max_regen: int = 5
) -> list[dict]:
    """
    Th·ª≠ l√†m m·ªõi t·ªëi ƒëa max_regen c√¢u k√©m ch·∫•t l∆∞·ª£ng b·∫±ng generate_one_question.
    items: list c√°c b·∫£n ghi {"question_text","type","options","score","issues"}
    """
    out = []
    regen_count = 0
    for it in items:
        qtext = it["question_text"]
        qtype = it["type"]
        issues = it.get("issues", [])

        if regen_count < max_regen and ("placeholder_text" in issues or it.get("score", 0) < 65):
            # chu·∫©n b·ªã prompt ng·∫Øn, c·ª• th·ªÉ ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng
            hint = (
                "T·∫°o 1 c√¢u h·ªèi r√µ r√†ng, t·ª± nhi√™n, tr√°nh placeholder. "
                "N·∫øu l√† multiple_choice/single_choice/ranking: sinh 3‚Äì5 l·ª±a ch·ªçn th·ª±c t·∫ø; "
                "v·ªõi date_time/file_upload: KH√îNG c√≥ options. Ch·ªâ tr·∫£ JSON."
            )
            one = client.generate_one_question(
                title=title,
                category=category or "general",
                question_type=qtype,
                ai_prompt=hint,
                previous_question=qtext or "placeholder"
            )
            if one and one.get("question_text"):
                # gh√©p l·∫°i record m·ªõi, gi·ªØ type ƒë√£ c√¢n b·∫±ng
                out.append({
                    "question_text": one["question_text"].strip(),
                    "type": qtype,
                    "options": one.get("options")
                })
                regen_count += 1
                continue  # sang c√¢u ti·∫øp theo
        # n·∫øu kh√¥ng regen, gi·ªØ nguy√™n
        out.append({"question_text": qtext, "type": qtype, "options": it.get("options")})
    return out



def validate_question_text(q: str) -> List[str]:
    """Rule-based validation: ph√°t hi·ªán l·ªói ph·ªï bi·∫øn."""
    issues: List[str] = []
    if not q or len(q.strip()) < 8:
        issues.append("C√¢u h·ªèi qu√° ng·∫Øn/kh√¥ng r√µ.")
    low = q.lower()
    if q.lower().startswith("c√¢u h·ªèi "):
        issues.append("placeholder_text")

    if "t·∫°i sao b·∫°n kh√¥ng" in low or "c√≥ ph·∫£i" in low or "ƒë·ªìng √Ω r·∫±ng" in low:
        issues.append("C√¢u h·ªèi c√≥ th·ªÉ d·∫´n d·∫Øt/thi√™n ki·∫øn.")
    if "v√† t·∫°i sao" in low and len(q) > 180:
        issues.append("C√¢u h·ªèi qu√° d√†i ho·∫∑c g·ªôp nhi·ªÅu √Ω.")
    return issues

def score_question(q: str) -> int:
    """Ch·∫•m ƒëi·ªÉm ch·∫•t l∆∞·ª£ng [0..100] theo quy t·∫Øc ƒë∆°n gi·∫£n + chi·ªÅu d√†i h·ª£p l√Ω."""
    if not q:
        return 0
    score = 60
    ln = len(q.strip())
    if 40 <= ln <= 140:
        score += 20
    elif 20 <= ln < 40 or 140 < ln <= 220:
        score += 10
    penalty_words = ["c√≥ ph·∫£i", "r√µ r√†ng l√†", "ch·∫Øc ch·∫Øn", "ai c≈©ng bi·∫øt"]
    for w in penalty_words:
        if w in q.lower():
            score -= 10
    return max(0, min(100, score))

def build_prompt(req: SurveyGenerationRequest) -> str:
    category = normalize_category(getattr(req, "category_name", None))
    tmpl = INDUSTRY_TEMPLATES.get(category, DEFAULT_TEMPLATE)
    types_text = ", ".join(SUPPORTED_TYPES)

    # Over-generate c√≥ tr·∫ßn ƒë·ªÉ tr√°nh b·ªÉ token JSON:
    base_n = max(1, int(req.number_of_questions or 1))
    cap = 20 
    N_over = min(max(base_n, math.ceil(base_n * 1.6)), cap)

    extra = []
    if getattr(req, "ai_prompt", None):
        extra.append(req.ai_prompt.strip())
    if getattr(req, "description", None):
        extra.append(req.description.strip())
    context = "\nNg·ªØ c·∫£nh b·ªï sung t·ª´ ng∆∞·ªùi d√πng:\n" + "\n".join(extra) if extra else ""

    final_prompt = tmpl.format(
        n=N_over,
        title=req.title,
        types=types_text
    ) + context

    # S·ª≠ d·ª•ng priorities t·ª´ request n·∫øu c√≥
    priorities = getattr(req, "question_type_priorities", None)
    target_types = choose_target_types(N_over, priorities)
    
    if priorities:
        final_prompt += f"\n∆ØU TI√äN c√°c lo·∫°i c√¢u h·ªèi theo th·ª© t·ª±: {', '.join(priorities[:5])}."
    final_prompt += f"\nPh√¢n b·ªï lo·∫°i c√¢u h·ªèi THEO TH·ª® T·ª∞: {target_types}."
    final_prompt += "\nCH·ªà tr·∫£ JSON h·ª£p l·ªá (kh√¥ng markdown, kh√¥ng ch√∫ th√≠ch)."
    final_prompt += "\nM·ªói c√¢u h·ªèi ph·∫£i c·ª• th·ªÉ, KH√îNG d√πng placeholder nh∆∞ 'C√¢u h·ªèi 1'."

    return final_prompt

def generate_many_questions(client: GeminiClient, req: SurveyGenerationRequest) -> List[Dict[str, Any]]:
    target_total = max(1, req.number_of_questions)
    collected: List[Dict[str, Any]] = []
    seen = set()  # tr√°nh tr√πng

    # batch c·ªë ƒë·ªãnh 8‚Äì12 c√¢u/l·∫ßn (tr√°nh MAX_TOKENS)
    BATCH_CAP = 12

    while len(collected) < target_total:
        remain = target_total - len(collected)
        # m·ªói batch xin d∆∞ 1.5x nh∆∞ng cap 12
        ask_n = min(max(remain, ceil(remain * 1.5)), BATCH_CAP)

        # t·∫°m clone req v·ªõi s·ªë c√¢u = ask_n
        tmp_req = SurveyGenerationRequest(**{**req.dict(), "number_of_questions": ask_n})
        prompt = build_prompt(tmp_req)

        gem_res = client.generate_survey(prompt=prompt, context=tmp_req.description or "")
        if not gem_res or not getattr(gem_res, "questions", None):
            # n·∫øu batch n√†y r·ªóng, tho√°t s·ªõm ƒë·ªÉ tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n
            break

        # ch·∫•m ƒëi·ªÉm + l·ªçc tr√πng + l·ªçc placeholder + √©p options t·ªëi thi·ªÉu
        batch = []
        for q in gem_res.questions:
            q_text = (q.question_text or "").strip()
            key = q_text.lower()
            if not q_text or key.startswith("c√¢u h·ªèi "):
                continue
            if key in seen:
                continue
            q_type = _normalize_type(getattr(q, "question_type", getattr(q, "type", "open_ended")))
            options = getattr(q, "options", None)
            if q_type in ("multiple_choice", "single_choice", "ranking"):
                opts = _coerce_option_list(options)
                if q_type == "ranking" and len(opts) < 5:
                    need = 5 - len(opts)
                    opts += [f"M·ª•c {i}" for i in range(1, need + 1)]
                options = opts
            else:
                options = None

            issues = validate_question_text(q_text)
            score = score_question(q_text)
            batch.append({"question_text": q_text, "type": q_type, "options": options, "score": score, "issues": issues})

        # ∆∞u ti√™n c√¢u kh√¥ng issue + ƒëi·ªÉm cao
        batch.sort(key=lambda x: (len(x["issues"]) == 0, x["score"]), reverse=True)

        # l·∫•y v·ª´a ƒë·ªß ph·∫ßn c√≤n thi·∫øu
        for it in batch:
            if len(collected) >= target_total:
                break
            if it["issues"] or it["score"] < 65:
                continue  # gi·ªØ ch·∫•t l∆∞·ª£ng
            collected.append(it)
            seen.add(it["question_text"].lower())

        # n·∫øu v·∫´n thi·∫øu nhi·ªÅu, v√≤ng while s·∫Ω xin batch ti·∫øp theo

    return collected

def _coerce_option_list(opts: Any) -> List[str]:
    """Chu·∫©n ho√° options v·ªÅ list[str]."""
    if not opts:
        return []
    if isinstance(opts, list):
        return [str(x).strip() for x in opts if str(x).strip()]
    # ƒë√¥i khi model tr·∫£ string "A;B;C"
    if isinstance(opts, str):
        parts = [p.strip() for p in opts.split(";")]
        return [p for p in parts if p]
    return []

def _normalize_type(t: str) -> str:
    t = (t or "").strip().lower().replace("-", "_").replace(" ", "_")
    if t in SUPPORTED_TYPES:
        return t
    if t in ["singlechoice", "single", "radio"]:
        return "single_choice"
    if t in ["mcq", "multiple", "multi_choice", "multi"]:
        return "multiple_choice"
    if t in ["boolean", "yes_no"]:
        return "boolean_"
    if t in ["date", "datetime", "date_time", "time", "time_picker"]:
        return "date_time"
    if t in ["file", "upload", "file_upload", "attachment"]:
        return "file_upload"
    return t or "open_ended"

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:8080"],  # URL Frontend
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Global Gemini client
_gemini_client: GeminiClient = None

def get_gemini_client() -> GeminiClient:
    """Dependency ƒë·ªÉ l·∫•y Gemini client"""
    global _gemini_client
    if _gemini_client is None:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise HTTPException(
                status_code=500,
                detail="Ch∆∞a c·∫•u h√¨nh Gemini API key"
            )
        _gemini_client = create_gemini_client(api_key)
    return _gemini_client

@app.on_event("startup")
async def startup_event():
    import logging
    logging.getLogger("app.core.gemini_client").setLevel(logging.DEBUG)
    logging.getLogger().setLevel(logging.INFO)  # gi·ªØ INFO cho global, DEBUG ri√™ng gemini_client

    """Kh·ªüi t·∫°o c√°c d·ªãch v·ª• khi startup"""
    logger.info("D·ªãch v·ª• T·∫°o Kh·∫£o s√°t ƒëang kh·ªüi ƒë·ªông...")
    
    # X√°c minh Gemini API key c√≥ s·∫µn
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        logger.warning("Ch∆∞a thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY")
    else:
        logger.info("ƒê√£ c·∫•u h√¨nh th√†nh c√¥ng Gemini API key")

@app.get("/health")
async def health_check():
    """Endpoint ki·ªÉm tra s·ª©c kh·ªèe d·ªãch v·ª•"""
    return {
        "status": "healthy",
        "service": "survey-generator",
        "version": "1.0.0",
        "ok" : True
    }

@app.get("/templates")
def list_templates():
    return {
        "default": True,
        "supported_types": SUPPORTED_TYPES,
        "industries": list(INDUSTRY_TEMPLATES.keys())
    }

@app.post("/generate", response_model=SurveyGenerationResponse)
def generate_survey(request: SurveyGenerationRequest):
    try:
        logger.info(f"üì• Received generation request: N={request.number_of_questions}, priorities={request.question_type_priorities}")
        logger.info(f"üîç Request details: title={request.title}, category={request.category_name}")
        logger.info(f"üîç Full request dict: {request.model_dump()}")
        
        client: GeminiClient = get_gemini_client()
        N = int(request.number_of_questions)

        # ‚úÖ CRITICAL FIX: N·∫øu c√≥ priorities, B·ªé QUA BIG PROMPT, ch·ªâ d√πng parallel
        priorities = request.question_type_priorities
        if priorities and len(priorities) > 0:
            logger.info(f"üéØ Priority mode enabled! Bypassing BIG PROMPT, using parallel generation only.")
            # T·∫°o tr·ª±c ti·∫øp v·ªõi parallel ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√∫ng type
            parsed = parallel_generate_exact_n(client, request)
            gem_res = None  # Kh√¥ng d√πng BIG PROMPT
        else:
            # 1) G·ªçi AI v·ªõi BIG PROMPT (ch·ªâ khi kh√¥ng c√≥ priorities)
            logger.info("üìù No priorities, using BIG PROMPT mode")
            prompt = build_prompt(request)
            gem_res = client.generate_survey(prompt=prompt, context=request.description or "")

        # ---- Helpers n·ªôi b·ªô ----
        def _score_and_normalize(q_obj, idx: int) -> dict:
            q_text = (getattr(q_obj, "question_text", "") or "").strip()
            q_type = _normalize_type(getattr(q_obj, "type", getattr(q_obj, "question_type", "open_ended")))
            opts = getattr(q_obj, "options", None)

            if q_type in ("multiple_choice", "single_choice", "ranking"):
                options = _coerce_option_texts(opts)
                if q_type == "ranking" and len(options) < 5:
                    need = 5 - len(options)
                    options += [f"M·ª•c {k}" for k in range(1, need + 1)]
            else:
                options = []

            issues = validate_question_text(q_text)
            score = score_question(q_text)
            return {
                "question_text": q_text or f"C√¢u h·ªèi {idx}",
                "type": q_type,
                "options": options,
                "score": score,
                "issues": issues
            }

        def _dedup_merge(base: list[dict], extra: list[dict]) -> list[dict]:
            merged, seen = [], set()
            for x in (base + extra):
                qt = (x.get("question_text") or "").strip()
                key = qt.lower()
                if not qt or key in seen:
                    continue
                seen.add(key)
                merged.append(x)
            return merged

        def _pick_valid(items: list[dict], limit: int) -> list[dict]:
            # sort theo score cao & √≠t issues
            items_sorted = sorted(items, key=lambda x: (x.get("score", 0), -len(x.get("issues", []))), reverse=True)
            out = []
            for it in items_sorted:
                if len(out) >= limit:
                    break
                qt = (it["question_text"] or "").strip().lower()
                # lo·∫°i placeholder "c√¢u h·ªèi ..." ho·∫∑c r·ªóng
                if not qt or qt.startswith("c√¢u h·ªèi "):
                    continue
                if it.get("score", 0) >= MIN_SCORE and not it.get("issues"):
                    out.append(it)
            return out

        # 2) Parse -> ch·∫•m ƒëi·ªÉm -> build 'parsed' (ch·ªâ khi d√πng BIG PROMPT)
        if not priorities:
            # BIG PROMPT mode: parse k·∫øt qu·∫£
            if gem_res and getattr(gem_res, "questions", None):
                for i, q in enumerate(gem_res.questions, start=1):
                    parsed.append(_score_and_normalize(q, i))

            # 2b) Lo·∫°i MCQ/Single thi·∫øu options (tr√°nh fallback A/B/C)
            cleaned = []
            for it in parsed:
                qtype = it.get("type")
                opts = it.get("options") or []
                if qtype in ("multiple_choice", "single_choice"):
                    if len([o for o in opts if (o or "").strip()]) < 2:
                        continue
                cleaned.append(it)
            parsed = cleaned

            # 3) N·∫øu thi·∫øu ho·∫∑c >50% k√©m ‚Üí sinh song song ƒë·ªÉ B√ô ƒê·ª¶ N (ƒë·ª£t 1)
            need_more = N - sum(1 for x in parsed if (x["score"] >= MIN_SCORE and not x["issues"]))
            too_many_bad = sum(1 for x in parsed if (x["score"] < MIN_SCORE or x["issues"])) > max(2, len(parsed) // 2)
            if need_more > 0 or too_many_bad:
                exact = parallel_generate_exact_n(client, request)
                parsed = _dedup_merge(parsed, exact)

            # 3b) LOOP fill cho ƒë·∫øn ƒë·ªß N (t·ªëi ƒëa 3 l·∫ßn) - ch·ªâ khi kh√¥ng c√≥ priorities
            attempts = 0
            while attempts < 3:
                valid_now = _pick_valid(parsed, N)
                remaining = N - len(valid_now)
                if remaining <= 0:
                    break

                # G·ªçi b√π ƒë√∫ng s·ªë c√≤n thi·∫øu
                req_rem = SurveyGenerationRequest(**{**request.dict(), "number_of_questions": remaining})
                extra = parallel_generate_exact_n(client, req_rem)
                parsed = _dedup_merge(parsed, extra)
                attempts += 1
        # else: priorities mode ƒë√£ c√≥ parsed t·ª´ parallel_generate_exact_n()

        # 4) Ch·ªçn Top-N theo score & kh√¥ng placeholder
        topN = _pick_valid(parsed, N)

        # 5) Ki·ªÉm tra s·ªë l∆∞·ª£ng c√¢u h·ªèi h·ª£p l·ªá - KH√îNG D√ôNG FALLBACK C·ª®
        if not topN:
            logger.error(f"‚ùå AI kh√¥ng sinh ƒë∆∞·ª£c c√¢u h·ªèi h·ª£p l·ªá n√†o. Parsed: {len(parsed)}, Valid: 0")
            return SurveyGenerationResponse(
                success=False,
                message="AI kh√¥ng th·ªÉ t·∫°o c√¢u h·ªèi ph√π h·ª£p. Vui l√≤ng th·ª≠ l·∫°i v·ªõi prompt r√µ r√†ng h∆°n ho·∫∑c gi·∫£m s·ªë l∆∞·ª£ng c√¢u h·ªèi.",
                error_details={
                    "hint": "no_valid_questions",
                    "parsed_count": len(parsed),
                    "suggestions": [
                        "Th·ª≠ gi·∫£m s·ªë l∆∞·ª£ng c√¢u h·ªèi (3-5 c√¢u)",
                        "L√†m r√µ h∆°n prompt/m√¥ t·∫£",
                        "Ki·ªÉm tra k·∫øt n·ªëi API"
                    ]
                }
            )
        
        # 5b) N·∫øu thi·∫øu m·ªôt √≠t (< 50%), c·∫£nh b√°o nh∆∞ng v·∫´n tr·∫£ v·ªÅ
        if len(topN) < N:
            shortage = N - len(topN)
            logger.warning(f"‚ö†Ô∏è Ch·ªâ sinh ƒë∆∞·ª£c {len(topN)}/{N} c√¢u h·ªèi h·ª£p l·ªá. Thi·∫øu {shortage} c√¢u.")
            # Kh√¥ng pad th√™m c√¢u r√°c, ch·ªâ tr·∫£ v·ªÅ s·ªë c√≥ ƒë∆∞·ª£c
        
        # 5c) N·∫øu qu√° √≠t c√¢u (< 50% y√™u c·∫ßu), tr·∫£ l·ªói thay v√¨ tr·∫£ c√¢u r√°c
        if len(topN) < max(3, N // 2):
            logger.error(f"‚ùå Ch·ªâ sinh ƒë∆∞·ª£c {len(topN)}/{N} c√¢u ({len(topN)*100//N}%), qu√° th·∫•p!")
            return SurveyGenerationResponse(
                success=False,
                message=f"Ch·ªâ t·∫°o ƒë∆∞·ª£c {len(topN)}/{N} c√¢u h·ªèi h·ª£p l·ªá. Vui l√≤ng th·ª≠ l·∫°i v·ªõi y√™u c·∫ßu ƒë∆°n gi·∫£n h∆°n.",
                error_details={
                    "hint": "insufficient_quality",
                    "generated": len(topN),
                    "requested": N,
                    "suggestions": [
                        "Gi·∫£m s·ªë c√¢u h·ªèi xu·ªëng 3-5",
                        "Cung c·∫•p prompt c·ª• th·ªÉ h∆°n",
                        "Ki·ªÉm tra API key c√≤n quota"
                    ]
                }
            )

        # 6) C·∫Øt ƒë√∫ng N (kh√¥ng c·∫ßn t·ªëi thi·ªÉu 3 n·ªØa)
        topN = topN[:N]

        # 7) Build response theo schema Pydantic
        gen = GeneratedSurveyResponse(
            title=request.title,
            description=request.description,
            questions=[
                QuestionSchema(
                    question_text=it["question_text"],
                    question_type=it["type"],
                    is_required=True,
                    display_order=i + 1,
                    options=[{"option_text": o, "display_order": j + 1} for j, o in enumerate(it.get("options") or [])]
                )
                for i, it in enumerate(topN)
            ]
        )

        return SurveyGenerationResponse(
            success=True,
            survey_id=None,
            message="T·∫°o c√¢u h·ªèi th√†nh c√¥ng",
            generated_survey=gen
        )

    except GeminiOverloadedError:
        raise HTTPException(status_code=503, detail="AI provider ƒëang qu√° t·∫£i (503). Vui l√≤ng th·ª≠ l·∫°i.")
    except Exception as e:
        logger.error(f"‚ùå L·ªñI KHI T·∫†O KH·∫¢O S√ÅT: {type(e).__name__}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))





@app.get("/categories")
async def get_categories():
    """L·∫•y danh s√°ch c√°c danh m·ª•c kh·∫£o s√°t c√≥ s·∫µn"""
    return {
        "categories": [
            {"id": k, "name": v} for k, v in CATEGORY_MAPPING.items()
        ]
    }

@app.get("/validate-prompt")
async def validate_prompt(prompt: str):
    """Ki·ªÉm tra xem prompt c√≥ ph√π h·ª£p ƒë·ªÉ t·∫°o kh·∫£o s√°t kh√¥ng"""
    try:
        if len(prompt.strip()) < 10:
            return {
                "valid": False,
                "message": "Prompt qu√° ng·∫Øn. Vui l√≤ng cung c·∫•p m√¥ t·∫£ chi ti·∫øt h∆°n."
            }
        
        if len(prompt.strip()) > 1000:
            return {
                "valid": False,
                "message": "Prompt qu√° d√†i. Vui l√≤ng r√∫t g·ªçn l·∫°i."
            }
        
        survey_keywords = [
            "kh·∫£o s√°t", "survey", "c√¢u h·ªèi", "ph·∫£n h·ªìi", "ƒë√°nh gi√°", 
            "√Ω ki·∫øn", "th·ªëng k√™", "ph·ªèng v·∫•n", "nghi√™n c·ª©u"
        ]
        
        prompt_lower = prompt.lower()
        has_survey_context = any(keyword in prompt_lower for keyword in survey_keywords)
        
        if not has_survey_context:
            return {
                "valid": True,
                "message": "Prompt h·ª£p l·ªá nh∆∞ng n√™n b·ªï sung th√™m ng·ªØ c·∫£nh kh·∫£o s√°t.",
                "suggestions": [
                    "Th√™m t·ª´ kh√≥a nh∆∞ 'kh·∫£o s√°t', 'ƒë√°nh gi√°', '√Ω ki·∫øn'",
                    "M√¥ t·∫£ r√µ m·ª•c ƒë√≠ch thu th·∫≠p th√¥ng tin",
                    "Ch·ªâ ƒë·ªãnh ƒë·ªëi t∆∞·ª£ng kh·∫£o s√°t"
                ]
            }
        
        return {
            "valid": True,
            "message": "Prompt ph√π h·ª£p ƒë·ªÉ t·∫°o kh·∫£o s√°t"
        }
        
    except Exception as e:
        logger.error(f"L·ªói khi ki·ªÉm tra prompt: {str(e)}")
        return {
            "valid": False,
            "message": "L·ªói khi ki·ªÉm tra prompt"
        }

# C·∫£i thi·ªán th·ªùi gian resfresh 1 question 
class RefreshQuestionRequest(BaseModel):
    title: str
    category: str | None = None
    question_type: str  # "single_choice" | "multiple_choice" | "ranking" | "rating" | "open_ended" | "boolean"
    ai_prompt: str | None = None          # g·ª£i √Ω th√™m c·ªßa user
    previous_question: str | None = None  # c√¢u c≈© ƒë·ªÉ ‚Äúrephrase/refresh‚Äù (optional)
    previous_options: list[str] | None = None  # options c≈© (n·∫øu c√≥), ƒë·ªÉ model tr√°nh l·∫∑p (optional)

class RefreshQuestionResponse(BaseModel):
    success: bool
    question_text: str | None = None
    question_type: str | None = None
    options: list[str] | None = None
    message: str | None = None

def build_single_question_prompt(req: RefreshQuestionRequest) -> str:
    cat = normalize_category(req.category)
    base = INDUSTRY_TEMPLATES.get(cat, DEFAULT_TEMPLATE)

    guide = f"""
    Ch·ªâ t·∫°o 1 c√¢u h·ªèi lo·∫°i: {req.question_type}.
    Y√™u c·∫ßu:
    - R√µ r√†ng, kh√¥ng d·∫´n d·∫Øt
    - N·∫øu l√† single_choice/multiple_choice: sinh 5-7 l·ª±a ch·ªçn, kh√¥ng tr√πng l·∫∑p
    - N·∫øu l√† ranking: sinh 5-7 m·ª•c ƒë·ªÉ x·∫øp h·∫°ng
    - N·∫øu l√† rating: d√πng thang 1-5
    - N·∫øu l√† date_time: ch·ªâ ƒë·ªãnh ng√†y/gi·ªù; kh√¥ng c√≥ options
    - N·∫øu l√† file_upload: kh√¥ng c√≥ options; m√¥ t·∫£ lo·∫°i t·ªáp n·∫øu c·∫ßn
    - Tr·∫£ v·ªÅ JSON t·ªëi gi·∫£n: question_text, type, options (n·∫øu c√≥)

    Ng·ªØ c·∫£nh survey: "{req.title}"
    """
    if req.previous_question:
        guide += f"\nTr√°nh l·∫∑p l·∫°i/di·ªÖn ƒë·∫°t l·∫°i qu√° gi·ªëng c√¢u tr∆∞·ªõc: \"{req.previous_question}\""
    if req.previous_options:
        joined = "; ".join(req.previous_options)
        guide += f"\nKh√¥ng l·∫∑p l·∫°i c√°c l·ª±a ch·ªçn tr∆∞·ªõc: {joined}"

    if req.ai_prompt:
        guide += f"\nG·ª£i √Ω b·ªï sung: {req.ai_prompt.strip()}"

    return f"T·∫°o 1 c√¢u h·ªèi ch·∫•t l∆∞·ª£ng cao cho survey \"{req.title}\".\n{guide}\nCh·ªâ tr·∫£ JSON."

@app.post("/refresh_question", response_model=RefreshQuestionResponse)
def refresh_question(req: RefreshQuestionRequest):
    """
    Sinh l·∫°i 1 c√¢u (nhanh), ch·∫•p nh·∫≠n ƒë·∫ßu ra ƒë√¥i khi l·ªôn x·ªôn v√† √©p v·ªÅ schema FE c·∫ßn.
    Kh√¥ng c√≤n l·ªói 'str'.get v√¨ lu√¥n ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu tr∆∞·ªõc khi truy c·∫≠p.
    """
    try:
        client: GeminiClient = get_gemini_client()

        raw = client.generate_one_question(
            title=req.title or "",
            category=(req.category or "general"),
            question_type=req.question_type,
            ai_prompt=req.ai_prompt or "",
            previous_question=req.previous_question or "",
        )

        # 1) B·∫£o v·ªá ki·ªÉu d·ªØ li·ªáu
        if not isinstance(raw, dict):
            return RefreshQuestionResponse(success=False, message="AI kh√¥ng tr·∫£ v·ªÅ JSON h·ª£p l·ªá.")

        # 2) L·∫•y tr∆∞·ªùng v·ªõi nhi·ªÅu bi·∫øn th·ªÉ, √©p lo·∫°i & √©p options
        q_text = (raw.get("question_text") or raw.get("text") or raw.get("question") or "").strip()
        q_type = _normalize_type(raw.get("question_type") or raw.get("type") or "open_ended")

        if not q_text:
            return RefreshQuestionResponse(success=False, message="AI kh√¥ng tr·∫£ v·ªÅ c√¢u h·ª£p l·ªá.")

        options = []
        if q_type in ("multiple_choice", "single_choice", "ranking"):
            options = _coerce_option_texts(raw.get("options"))
            # ranking c·∫ßn t·ªëi thi·ªÉu 5 m·ª•c ƒë·ªÉ x·∫øp h·∫°ng m∆∞·ª£t
            if q_type == "ranking" and len(options) < 5:
                need = 5 - len(options)
                options += [f"M·ª•c {i}" for i in range(1, need + 1)]
        else:
            options = []

        return RefreshQuestionResponse(
            success=True,
            question_text=q_text,
            question_type=q_type,
            options=options or None
        )

    except GeminiOverloadedError:
        raise HTTPException(status_code=503, detail="AI provider ƒëang qu√° t·∫£i (503). Vui l√≤ng th·ª≠ l·∫°i.")
    except Exception as e:
        # Tr·∫£ message r√µ r√†ng cho FE
        return RefreshQuestionResponse(success=False, message=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8002,
        reload=True
    )